{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pr-shukla/Pursuit-Evasion/blob/main/pursuit_evader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4Wt6ge5uEAf"
   },
   "source": [
    "# **Importing required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOOvM2I6m9Gn",
    "outputId": "bbf1bcb3-f8d5-4e61-a349-27a9231d4101"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "import pylab\n",
    "!pip install pyyaml h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZypnMIVua5-"
   },
   "source": [
    "# Defining Paramters for Engagement of Pursuer and Evader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BERlwrb2nDpn"
   },
   "outputs": [],
   "source": [
    "#Dimension of State Space\n",
    "dim_state = 3\n",
    "\n",
    "#hidden_nodes = 3\n",
    "\n",
    "#Time Difference Between 2 Steps\n",
    "dt = 0.1\n",
    "\n",
    "#Number of Episodes\n",
    "num_episodes = 1000\n",
    "\n",
    "#Number of Steps\n",
    "num_steps = 400\n",
    "\n",
    "#buffer_range = 60\n",
    "\n",
    "#Minimum turing radius of Pursuer\n",
    "rho = 1\n",
    "\n",
    "#velocity of pursuer\n",
    "v = 1.0\n",
    "\n",
    "\n",
    "#Velocity of Evader during training\n",
    "ve = 0.5\n",
    "\n",
    "#angle between initial velocity and reference\n",
    "te = 3*np.pi/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdIhGCG3u24c"
   },
   "source": [
    "# Defining Function to add random noise in actions to increase Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JF9BVmiXnGJI"
   },
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=0.1, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZmE6-vqvK-A"
   },
   "source": [
    "# Defining class **Buffer** to sample minibatch randomly and to update actors and critic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOxsR394nJBe"
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=10000, batch_size=64):\n",
    "\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "        #Buffer for critic / actor loss-function\n",
    "        self.loss_critic = 0\n",
    "        self.loss_actor = 0\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, dim_state))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, dim_state))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "            \n",
    "            self.loss_critic = critic_loss\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "            \n",
    "            self.loss_actor = actor_loss\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "    def get_list_critic(self):\n",
    "        return self.loss_critic\n",
    "    \n",
    "    def get_list_actor(self):\n",
    "        return self.loss_actor\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(tau):\n",
    "    new_weights = []\n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_critic.set_weights(new_weights)\n",
    "\n",
    "    new_weights = []\n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_actor.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps4p9t0mvhaZ"
   },
   "source": [
    "# Creating Actor and Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwygbdBynMDe"
   },
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.00003, maxval=0.00003)\n",
    "\n",
    "    inputs = layers.Input(shape=(dim_state,))\n",
    "    out = layers.Dense(256, activation=\"selu\", kernel_initializer=\"lecun_normal\")(inputs)\n",
    "    out = layers.Dropout(rate=0.5)(out )\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(256, activation=\"selu\", kernel_initializer=\"lecun_normal\")(out)\n",
    "    out = layers.Dropout(rate=0.5)(out )\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(256, activation=\"selu\", kernel_initializer=\"lecun_normal\")(inputs)\n",
    "    out = layers.Dropout(rate=0.5)(out )\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(256, activation=\"selu\", kernel_initializer=\"lecun_normal\")(out)\n",
    "    out = layers.Dropout(rate=0.5)(out )\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(dim_state))\n",
    "    state_out = layers.Dense(16, activation=\"selu\", kernel_initializer=\"lecun_normal\")(state_input)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"selu\", kernel_initializer=\"lecun_normal\")(state_out)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"selu\", kernel_initializer=\"lecun_normal\")(action_input)\n",
    "    action_out = layers.BatchNormalization()(action_out)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(512, activation=\"selu\", kernel_initializer=\"lecun_normal\")(concat)\n",
    "    out = layers.Dropout(rate=0.5)(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"selu\", kernel_initializer=\"lecun_normal\")(out)\n",
    "    out = layers.Dropout(rate=0.5)(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKfmLaPlvtlV"
   },
   "source": [
    "# Defining Function to Execute Actor Modelto generate actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7tOYoVmnPg1"
   },
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    \n",
    "    sampled_actions = tf.squeeze(actor_model(state ))\n",
    "    noise = noise_object()\n",
    "\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, -1.0, 1.0)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nky85ILFwHW4"
   },
   "source": [
    "# Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVqBfUG9nTny"
   },
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "actor_model.summary()\n",
    "plot_model(actor_model, to_file='actor_model.png')\n",
    "SVG(model_to_dot(actor_model).create(prog='dot', format='svg'))\n",
    "critic_model = get_critic()\n",
    "critic_model.summary()\n",
    "plot_model(critic_model, to_file='critic_model.png')\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models cr 0.0001 ac 0.00005\n",
    "critic_lr = 0.0001\n",
    "actor_lr = 0.00005\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 1000\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.98\n",
    "# Used to update target networks\n",
    "tau = 0.01\n",
    "\n",
    "buffer = Buffer(10000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYYFmeOOwO8J"
   },
   "source": [
    "# Defining Functions for dynamics of Pursuit EVader Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0jt3D38QRWW"
   },
   "outputs": [],
   "source": [
    "critic_lr = 0.00001\n",
    "actor_lr = 0.000005\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-UM7TOwQWJ1"
   },
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "dt = 0.1\n",
    "\n",
    "\n",
    "#cor0_e = [np.random.uniform(-3,3) , np.random.uniform(-1,5)] #начальные координаты цели x0_e, y0_e\n",
    "cor0_p = [0, 0, pi/2]\n",
    "Vx_e = -0.4\n",
    "Vy_e = 0.5\n",
    "\n",
    "def xEn(t, cor0):\n",
    "    return Vx_e * t + cor0[0]\n",
    "\n",
    "def yEn(t, cor0):\n",
    "    return Vy_e * t + cor0[1]\n",
    "\n",
    "# state[0] - x_p\n",
    "# state[1] - y_p\n",
    "# state[2] - угол поворота\n",
    "# state[3] - x_e\n",
    "# state[4] - y_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTBjx1xtQRmk"
   },
   "outputs": [],
   "source": [
    "def reward(state):\n",
    "    r = -np.log(10 * leng(state)) - (leng(state))**2\n",
    "    return r\n",
    "\n",
    "def leng(state):\n",
    "    length = (( state[0] - state [3])**2 + (state[1] - state[4])**2)**0.5\n",
    "    #length = (state[1]**2 + state[0]**2)**0.5\n",
    "    return length\n",
    "\n",
    "def transition(state, u , t , cor0):\n",
    "    new_state = (state[0] + np.cos(state[2]) * dt , state[1] + np.sin(state[2]) *dt, (state[2] + u*dt) % (2*np.pi) , xEn(t+dt, cor0), yEn(t+dt , cor0))\n",
    "    #new_state = (state[0] + (-1 + u*state[0] + np.cos(state[2])) * dt, state[1] + (-u*state[1] + np.sin(state[2])) * dt,state[2] + (1 - u  ) * dt)\n",
    "    \n",
    "    # угол прямой соединящей P и Е относительно оси Х\n",
    "    alpha_0 = angle(state)\n",
    "    # угол от нового положения\n",
    "    alpha = angle(new_state)\n",
    "    #(фи - альфа) -> угол отклонения (радиальное отклонение) / фи - угол между скоростью и осью Х\n",
    "    red_tet = new_state[2] - alpha\n",
    "    #редуцированное пространство / второй член - угловая скорость\n",
    "    red_state = (leng(state), (alpha-alpha_0)/dt, red_tet)\n",
    "    return new_state, red_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-F1-D8do2XE"
   },
   "outputs": [],
   "source": [
    "# угол между прямой соединящей (P и Е) и оси Х\n",
    "def angle(state):\n",
    "    return (math.atan2(state[4] - state[1],state[3] - state[0]) % (2*np.pi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2Vy5exiUvbY",
    "outputId": "096747f2-9003-4052-a1d5-6d80193f4439"
   },
   "outputs": [],
   "source": [
    "angle((0,0,pi/2 ,2, 3)) % (2*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma3__ViLwo-j"
   },
   "source": [
    "# Generating Episodes to train model\n",
    "**Episodes during training may not appear very nice. But once the model get trained and when we excute that model in next cell you will see the better results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tQbaYeGGnaD7",
    "outputId": "be00d2c7-11ec-4f01-9677-3ec8d796a9a0"
   },
   "outputs": [],
   "source": [
    "ep_reward_list = []\n",
    "ep_actor_loss_list = []\n",
    "ep_critic_loss_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "#lists with actor's / critic's losses\n",
    "critic_loss_list = []\n",
    "actor_loss_list = []\n",
    "\n",
    "# Takes about 20 min to train\n",
    "for ep in range(num_episodes):\n",
    "    cor0 = (np.random.uniform(-3,3) , np.random.uniform(-3,3))\n",
    "    #w = np.random.uniform(-5,5)\n",
    "    #R = np.random.uniform(0,5)\n",
    "    sys_state = [0 , 0 , np.pi/2 , cor0[0] , cor0[1]]\n",
    "    alpha = angle(sys_state)\n",
    "    red_tet = sys_state[2] - alpha\n",
    "    red_state = (leng(sys_state), 0, red_tet)\n",
    "    episodic_reward = 0\n",
    "    episodic_loss_actor = 0\n",
    "    episodic_loss_critic = 0\n",
    "    \n",
    "    xc = []\n",
    "    yc = []\n",
    "    xce = []\n",
    "    yce = []\n",
    "    \n",
    "    t = 0\n",
    "    dt = 0.1\n",
    "    #while True:\n",
    "    for i in range(num_steps):\n",
    "      if leng(sys_state) > 0.2:\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(red_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "\n",
    "        # Recieve state and reward from environment.\n",
    "        new_state, new_red_state = transition(sys_state, float(action[0]), t, cor0)\n",
    "        rew = reward(new_state)\n",
    "\n",
    "        buffer.record((red_state , action, rew, new_red_state))\n",
    "        episodic_reward += rew\n",
    "        buffer.learn()\n",
    "        #print(new_red_state)\n",
    "        update_target(tau)\n",
    "        \n",
    "        actor_los = tf.reshape(buffer.get_list_actor(), []).numpy()\n",
    "        critic_los = tf.reshape(buffer.get_list_critic(), []).numpy()\n",
    "        \n",
    "        episodic_loss_actor += actor_los\n",
    "        episodic_loss_critic += critic_los\n",
    "        \n",
    "        t += dt\n",
    "        red_state = new_red_state\n",
    "        sys_state = new_state\n",
    "        xc.append(sys_state[0])\n",
    "        yc.append(sys_state[1])\n",
    "        \n",
    "        xce.append(sys_state[3])\n",
    "        yce.append(sys_state[4])\n",
    "      else:\n",
    "        print('The target has been captured')\n",
    "        break\n",
    "\n",
    " \n",
    "        \n",
    "    xc1 = [sys_state[3]]\n",
    "    yc1 = [sys_state[4]]\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    \n",
    "    ep_actor_loss_list.append(episodic_loss_actor)\n",
    "    ep_critic_loss_list.append(episodic_loss_critic)\n",
    "    \n",
    "    # Mean of last 40 episodes\n",
    "    avg_actor_loss = np.mean(ep_actor_loss_list[-num_episodes:])\n",
    "    avg_critic_loss = np.mean(ep_critic_loss_list[-num_episodes:])\n",
    "    \n",
    "    critic_loss_list.append(avg_critic_loss)\n",
    "    actor_loss_list.append(avg_actor_loss)\n",
    "    \n",
    "    avg_reward = np.mean(ep_reward_list[-num_episodes:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "    plt.plot(xc,yc)\n",
    "    plt.plot(xce,yce)\n",
    "    plt.plot(xc1,yc1,'.')\n",
    "    title = 'Episode Number', ep+1\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-mEmb69FA0X"
   },
   "outputs": [],
   "source": [
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actor_loss_list)\n",
    "plt.title(\"Actor Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Value Actor Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(critic_loss_list)\n",
    "plt.title(\"Critic Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Value Critic Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk52XSl79IQH"
   },
   "outputs": [],
   "source": [
    "actor_model.load_weights('./checkpoints/lr_0.001')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRbQiA4RZAqJ"
   },
   "outputs": [],
   "source": [
    "#actor_model.save_weights('./checkpoints/lr_0.002')\n",
    "#actor_model.load_weights('./checkpoints/my_checkpoint_26_01_400ep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKXFKtp4xjHW"
   },
   "source": [
    "# Executing our trained model here to analyze how good the parameters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3BygBKAuCEe"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "!git clone  https://github.com/shmax3/dubins\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import numpy as np\n",
    "from numpy import array, sin, cos, sqrt, arctan, arccos, arctan2, roots, append, real, pi, inf\n",
    "import math\n",
    "from scipy import integrate\n",
    "from dubins.planar_reachable_set import x_RS, y_RS, x_LR, y_LR, V_CS, V_CC_minus, V_CC_plus\n",
    "\n",
    "def arctg2(y, x):\n",
    "    return (2 * (y >= 0) - 1) * arccos(x / sqrt(x ** 2 + y ** 2)) + 2 * (y < 0) * pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP137yWI8OrA"
   },
   "outputs": [],
   "source": [
    "pos_vec = [[],[],[],[],[]] #задаем вектор, который будет содержать массивы движения машины Дубинса и цели\n",
    "\n",
    "Vx_e, Vy_e = 0.5, 0.5 #скорость цели\n",
    "V_e = sqrt(Vx_e ** 2 + Vy_e ** 2) #модуль скорости цели\n",
    "\n",
    "ε = 0.001 #эпсилон для определения времени\n",
    "\n",
    "cor0 = (-2.5, -0.25)\n",
    "#cor0 = (np.random.uniform(-3,3) , np.random.uniform(-1,5)) #начальные координаты цели x0_e, y0_e (та же позиция, что и раньше)\n",
    "cor0_e = cor0\n",
    "cor0_p = [0, 0, pi/2] #начальные координаты машины Дубинса\n",
    "\n",
    "V_p, U_p = 1, 1 #скорость машины Дубинса\n",
    "t0, dt = 0, 0.01 #начальное время и шаг времени в анимации\n",
    "#posP = array(cor0_p) #массив начальных координат машины Дубинса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOuNCnNI8O0L"
   },
   "outputs": [],
   "source": [
    "def xE(t):\n",
    "    return Vx_e * t + cor0_e[0]\n",
    "\n",
    "def yE(t):\n",
    "    return Vy_e * t + cor0_e[1]\n",
    "\n",
    "def ρB_CS(t, x, y):\n",
    "    τ1 = (arctg2(y, 1-abs(x)) + arccos(1/sqrt((1-abs(x))**2 + y**2))) % (2*pi)\n",
    "    τ2 = (arctg2(y, 1-abs(x)) - arccos(1/sqrt((1-abs(x))**2 + y**2))) % (2*pi)\n",
    "    τ = array([τ1, τ2, 0, t])\n",
    "    τ = τ[τ <= t]\n",
    "    ρ = sqrt((abs(x) - x_RS(τ, t))**2 + (y - y_RS(τ, t))**2)\n",
    "    return ρ.min()\n",
    "def ρB_CC(t, x, y):\n",
    "    a = -(y + sin(t/3))\n",
    "    b = 3*(1+abs(x)) + cos(t/3)\n",
    "    c = 3*y - sin(t/3)\n",
    "    d = -(1+abs(x)) + cos(t/3)\n",
    "    ξ = roots([a, b, c, d]) \n",
    "    τ = (t/3 - 2*arctan(real(ξ))) % (2*pi)\n",
    "    τ = append(τ[τ <= t], [0, t])\n",
    "    ρ = sqrt((abs(x) - x_LR(τ, t))**2 + (y - y_LR(τ, t))**2)\n",
    "    return ρ.min()\n",
    "def ρB(t, x, y):\n",
    "    return min(ρB_CS(t, x, y), ρB_CC(t, x, y))\n",
    "def convergent_estimate(t, xE, yE, v_max):\n",
    "    t = t + ρB(t, xE(t), yE(t))/(1 + v_max)\n",
    "    return t\n",
    "\n",
    "def conv_est(t, xE, yE, v, ε):\n",
    "    n = 0\n",
    "    t_prev = float('-inf')\n",
    "    δ = ε/(1 + v)\n",
    "    while t - t_prev > δ:\n",
    "        t_prev = t\n",
    "        n += 1\n",
    "        t = convergent_estimate(t, xE, yE, v)\n",
    "    return n, t\n",
    "N, T = conv_est(0, xE, yE, V_e, ε)\n",
    "V_cs = V_CS(xE(T), yE(T))\n",
    "V_cc_m = V_CC_minus(xE(T), yE(T))\n",
    "V_cc_p = V_CC_plus(xE(T), yE(T))\n",
    "def min_V(v_CS, v_CC_minus, v_CC_plus, T):\n",
    "    V_CS_T = abs(v_CS - T)\n",
    "    V_CC_minus_T = abs(v_CC_minus - T)\n",
    "    V_CC_plus_T = abs(v_CC_plus - T)\n",
    "    V_list = [V_CS_T, V_CC_minus_T, V_CC_plus_T]\n",
    "    minor = V_list[0]\n",
    "    for V in V_list:\n",
    "        if V < minor:\n",
    "            minor = V\n",
    "    return V_list.index(minor)\n",
    "def teta_CS_func(t):\n",
    "    state = sqrt(abs((1 - abs(xE(t))) ** 2 + yE(t) ** 2 - 1)) #может быть под корнем отрицательное число!\n",
    "    alfa_C = (1 - abs(xE(t)) + yE(t) * state) / (state ** 2 + 1)\n",
    "    alfa_S = (yE(t) - (1 - abs(xE(t))) * state) / (state ** 2 + 1)\n",
    "    if alfa_S >= 0:\n",
    "        teta_CS = arccos(alfa_C)\n",
    "    else:\n",
    "        teta_CS = 2 * pi - arccos(alfa_C)\n",
    "    return teta_CS\n",
    "\n",
    "teta_CS_func(T)\n",
    "def u_CS_func(t):\n",
    "    teta_CS = teta_CS_func(T) #находим teta_CS от Т\n",
    "    return np.where((t < teta_CS) & (xE(T) != 0), [-np.sign(xE(T))],\n",
    "                    (np.where((t < teta_CS) & (xE(T) == 0), [1.], [0])))\n",
    "    \n",
    "def teta_CC_pol(t):\n",
    "    alfa = (5 - (1 + abs(xE(t))) ** 2 - yE(t) ** 2) / 4\n",
    "    return arccos(((1 + abs(xE(t))) * (2 - alfa) +\n",
    "                               yE(t) * sqrt(1 - alfa ** 2))/((1 + abs(xE(t))) ** 2 + yE(t) ** 2))\n",
    "\n",
    "def teta_CC_min(t):\n",
    "    alfa = (5 - (1 + abs(xE(t))) ** 2 - yE(t) ** 2) / 4\n",
    "    return arccos(((1 + abs(xE(t))) * (2 - alfa) -\n",
    "                               yE(t) * sqrt(1 - alfa ** 2))/((1 + abs(xE(t))) ** 2 + yE(t) ** 2))\n",
    "def u_CC_pol_func(t):\n",
    "    teta_CC = teta_CC_pol(T) #находим teta_CC от Т\n",
    "    return np.where((t < teta_CC) & (xE(T) != 0), [np.sign(xE(T))],\n",
    "                    (np.where((t >= teta_CC) & (xE(T) != 0), [-np.sign(xE(T))],\n",
    "                              (np.where((t < teta_CC) & (xE(T) == 0), [1.], [-1.])))))\n",
    "\n",
    "def u_CC_min_func(t):\n",
    "    teta_CC = teta_CC_min(T) #находим teta_CC от Т\n",
    "    return np.where((t < teta_CC) & (xE(T) != 0), [np.sign(xE(T))],\n",
    "                    (np.where((t >= teta_CC) & (xE(T) != 0), [-np.sign(xE(T))],\n",
    "                              (np.where((t < teta_CC) & (xE(T) == 0), [1.], [-1.])))))\n",
    "def u_opt(t0):\n",
    "    index = min_V(V_cs, V_cc_m, V_cc_p, T)\n",
    "    if index == 0:    \n",
    "        return u_CS_func(t0)\n",
    "    elif index == 1:\n",
    "        return u_CC_min_func(t0)\n",
    "    elif index == 2:\n",
    "        return u_CC_pol_func(t0)\n",
    "def compute_len(posP,posE):\n",
    "    return sqrt((posP[0]-posE[0])**2+(posP[1]-posE[1])**2)\n",
    "\n",
    "def policy_test(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    sampled_actions = sampled_actions.numpy()\n",
    "    return [sampled_actions]\n",
    "\n",
    "pos_vec = [[],[],[],[],[]]\n",
    "index = min_V(V_cs, V_cc_m, V_cc_p, T)\n",
    "posP =  [0, 0, np.pi/2]\n",
    "t0 = 0\n",
    "dt  = 0.1\n",
    "while t0 <= T:\n",
    "    #print(posP)\n",
    "    posE = [xE(t0), yE(t0)]\n",
    "    #posE_arr = array(posE)\n",
    "    #print(u_opt(t))\n",
    "    velo = np.array([np.cos(posP[2]),np.sin(posP[2]),float(u_opt(t0))])\n",
    "    posP = posP + velo*dt\n",
    "    if compute_len(posP,posE)<=2*ε:\n",
    "        print('ez_game')\n",
    "    t0 += dt\n",
    "    #posP_arr = array(posP)\n",
    "    pos_vec[0].append(posP[0]) #xP\n",
    "    pos_vec[1].append(posP[1]) #yP\n",
    "#cor0 = (np.random.uniform(-10,10) , np.random.uniform(-10,10))\n",
    "sys_state = [0 , 0 , np.pi/2 , cor0[0] , cor0[1]]\n",
    "alpha = angle(sys_state)\n",
    "red_tet = sys_state[2] - alpha\n",
    "red_state = (leng(sys_state), 0, red_tet)\n",
    "episodic_reward = 0\n",
    "xc = []\n",
    "yc = []\n",
    "xce = []\n",
    "yce = []\n",
    "t = 0\n",
    "dt = 0.1\n",
    "#while True:\n",
    "for i in range(num_steps):\n",
    "    if leng(sys_state) > 0.2:\n",
    "      tf_prev_state = tf.expand_dims(tf.convert_to_tensor(red_state), 0)\n",
    "    \n",
    "      action = policy_test(tf_prev_state, ou_noise)\n",
    "        \n",
    "      new_state, new_red_state = transition(sys_state, float(action[0]), t, cor0)\n",
    "      rew = reward(new_state)\n",
    "      t += dt\n",
    "      red_state = new_red_state\n",
    "      sys_state = new_state\n",
    "      xc.append(sys_state[0])\n",
    "      yc.append(sys_state[1])\n",
    "        \n",
    "      xce.append(sys_state[3])\n",
    "      yce.append(sys_state[4])\n",
    "    else:\n",
    "      break\n",
    "xc1 = [sys_state[3]]\n",
    "yc1 = [sys_state[4]]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "ax.plot(xc, yc, color = 'darkorange', label = 'Машина Дубинса')\n",
    "ax.plot(pos_vec[0], pos_vec[1], color = 'green', label = 'Аналитическое решение')\n",
    "ax.plot(xce, yce, color = 'royalblue', label = 'Цель')\n",
    "#plt.title('Optimal trajectories')\n",
    "ax.plot(xc1, yc1, '.', color = 'red', label = 'Точка перехвата')\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show() \n",
    "\n",
    "\n",
    "#plt.plot(xc,yc)\n",
    "#plt.plot(xce,yce)\n",
    "#plt.plot(pos_vec[0],pos_vec[1])\n",
    "#plt.plot(xc1,yc1,'.')\n",
    "#title = 'Сomparison of trajectories'\n",
    "#plt.title(title)\n",
    "#plt.show()\n",
    "#print('Opt time:',len(np.array(pos_vec[0]))*0.1)\n",
    "print('Optimal time:', T)\n",
    "print('NN time:',len(xc)*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gDY_rSw8O3d"
   },
   "outputs": [],
   "source": [
    "\n",
    "cor0 = (np.random.uniform(-10,10) , np.random.uniform(-10,10))\n",
    "sys_state = [0 , 0 , np.pi/2 , cor0[0] , cor0[1]]\n",
    "episodic_reward = 0\n",
    "xc = []\n",
    "yc = []\n",
    "xce = []\n",
    "yce = []\n",
    "t = 0\n",
    "dt = 0.1\n",
    "#while True:\n",
    "for i in range(num_steps):\n",
    "    if leng(sys_state) > 0.2:\n",
    "      tf_prev_state = tf.expand_dims(tf.convert_to_tensor(to_reduction_state(sys_state)), 0)\n",
    "\n",
    "      action = policy(tf_prev_state, ou_noise)\n",
    "\n",
    "       # Recieve state and reward from environment.\n",
    "      new_state = transition( sys_state, float(action[0]),t , cor0)\n",
    "      rew = reward(new_state)       \n",
    "      episodic_reward += rew\n",
    "      t += dt\n",
    "      sys_state = new_state\n",
    "      xc.append(sys_state[0])\n",
    "      yc.append(sys_state[1])\n",
    "        \n",
    "      xce.append(sys_state[3])\n",
    "      yce.append(sys_state[4])\n",
    "    else:\n",
    "      break\n",
    "xc1 = [sys_state[3]]\n",
    "yc1 = [sys_state[4]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqSLwCcU8PCe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pursuit_evader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
